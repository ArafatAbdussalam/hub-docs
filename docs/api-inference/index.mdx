<!-- DISABLE-FRONTMATTER-SECTIONS -->

# ðŸ¤— Hosted Inference API

Test and evaluate, for free, over 80,000 publicly accessible machine learning models, or your own private models, via simple HTTP requests, with fast inference hosted on Hugging Face shared infrastructure.

<Tip>

The Inference API is free to use, and rate limited. If you need an inference solution for production, check out our [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) service. With Inference Endpoints, you can easily deploy any machine learning model on dedicated and fully managed infrastructure. Select the cloud, region, compute instance, autoscaling range and security level to match your model, latency, throughput, and compliance needs.
  
</Tip>

## Main features:

- Get predictions from **80,000+ Transformers models** (T5, Blenderbot, Bart, GPT-2, Pegasus\...)
- Switch from one model to the next by just switching the model ID
- Use built-in integrations with **over 20 Open-Source libraries** (spaCy, SpeechBrain, etc).
- Upload, manage and serve your **own models privately**
- Run Classification, Image Segmentation, Automatic Speech Recognition, NER, Conversational, Summarization, Translation, Question-Answering, Embeddings Extraction tasks
- Out of the box accelerated inference on **CPU** powered by Intel Xeon Ice Lake

## Third-party library models:

- The [Hub](https://huggingface.co) now supports many new libraries:

  - [SpaCy](https://spacy.io/), [AllenNLP](https://allennlp.org/),
  - [Speechbrain](https://speechbrain.github.io/),
  - [Timm](https://pypi.org/project/timm/) and [many others](https://huggingface.co/docs/hub/libraries)...

- Those models are enabled on the API thanks to some docker integration [api-inference-community](https://github.com/huggingface/huggingface_hub/tree/main/api-inference-community).

<Tip warning>

Please note however, that these models will not allow you ([tracking issue](https://github.com/huggingface/huggingface_hub/issues/85)):

- To get full optimization
- To run private models
- To get access to GPU inference

</Tip>

## If you are looking for custom support from the Hugging Face team

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>

## Hugging Face is trusted in production by over 10,000 companies

<img class="block dark:hidden !shadow-none !border-0 !rounded-none" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/companies-light.png" width="600">
<img class="hidden dark:block !shadow-none !border-0 !rounded-none" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/companies-dark.png" width="600">



